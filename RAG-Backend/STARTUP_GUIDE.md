# ğŸš€ Complete RAG System - Startup Guide

## Quick Start (3 Commands)

### **First Time Setup (While Online)**

Open 3 terminals and run:

```bash
# Terminal 1 - LLM Backend
cd LLM-Backend
myenv\Scripts\activate
python main.py

# Terminal 2 - RAG Backend (auto-downloads models ~175MB on first run)
cd RAG-Backend
start.bat

# Terminal 3 - Frontend
cd Frontend\next-app
npm run dev
```

**Wait for model caching to complete on first run (~2-3 minutes)**

Access: http://localhost:3000

---

## Subsequent Runs (Instant Startup)

Same commands, but no downloads - starts in seconds!

---

## âœ… Pre-Flight Check

Before starting, verify everything is ready:

```bash
cd RAG-Backend
test.bat
```

**Should show:**

```
ğŸ‰ All systems operational! Ready for offline demo.
```

---

## ğŸ¯ What Happens on First Run?

### **Terminal 2 (RAG Backend) Output:**

```
ğŸš€ RAG Backend - Model Caching (First Run Setup)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This will download ~175 MB of models for offline operation.
This only happens once. Please wait...

ğŸ“¥ Caching embedding model (all-MiniLM-L6-v2)...
   âœ… Embedding model cached (dim: 384)

ğŸ“¥ Caching reranker model (ms-marco-MiniLM-L-6-v2)...
   âœ… Reranker model cached

ğŸ“¥ Caching spaCy model (en_core_web_sm)...
   âœ… spaCy model downloaded and cached

ğŸ” Checking LLM model...
   âœ… LLM model found: 4.37 GB

ğŸ” Checking Tesseract OCR...
   âš ï¸  Tesseract OCR not found (optional for OCR features)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Model Cache Summary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… LLM Model: LLM model ready (4.37 GB)
âœ… Embedding Model: Embedding model ready
âœ… Reranker Model: Reranker model ready
âœ… spaCy Model: spaCy model ready
âš ï¸  Tesseract OCR: (optional)

ğŸ‰ All critical models cached! Ready for offline operation.

INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001
```

---

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Your Computer (Offline)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Frontend (Port 3000)                                       â”‚
â”‚    â†“                                                        â”‚
â”‚  RAG Backend (Port 8001)                                    â”‚
â”‚    â”œâ”€â”€ Embeddings (all-MiniLM-L6-v2) âœ… Cached            â”‚
â”‚    â”œâ”€â”€ Reranker (ms-marco) âœ… Cached                       â”‚
â”‚    â”œâ”€â”€ spaCy NER âœ… Cached                                 â”‚
â”‚    â””â”€â”€ Vector Store (FAISS) + Graph (rustworkx)            â”‚
â”‚    â†“                                                        â”‚
â”‚  LLM Backend (Port 8080)                                    â”‚
â”‚    â””â”€â”€ FastAPI Wrapper                                      â”‚
â”‚    â†“                                                        â”‚
â”‚  llama.cpp Server (Port 8000)                               â”‚
â”‚    â””â”€â”€ Mistral-7B (4.4 GB GGUF) âœ… Local                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

No Internet Required After First Setup! ğŸ‰
```

---

## ğŸ” Troubleshooting

### **Problem: Models won't download**

**Solution:**

```bash
# Check internet connection
ping google.com

# Manually cache
cd RAG-Backend
python cache_models.py
```

### **Problem: "Port already in use"**

**Solution:**

```bash
# Find process using port
netstat -ano | findstr :8001

# Kill process
taskkill /PID <PID> /F
```

### **Problem: Frontend won't start**

**Solution:**

```bash
cd Frontend\next-app
rm -rf node_modules package-lock.json
npm install
npm run dev
```

### **Problem: Want to verify everything**

**Solution:**

```bash
cd RAG-Backend
test.bat
```

### **Problem: LLM Backend not responding**

**Check:**

1. Is `main.py` running in LLM-Backend?
2. Is port 8080 free?
3. Is mistral model file present?

```bash
cd LLM-Backend
python main.py
```

---

## ğŸ“¦ Disk Space Required

| Component             | Size        | Location             |
| --------------------- | ----------- | -------------------- |
| Mistral-7B GGUF       | 4.4 GB      | LLM-Backend/         |
| Embedding Model       | 80 MB       | HF Cache             |
| Reranker Model        | 80 MB       | HF Cache             |
| spaCy Model           | 15 MB       | Python site-packages |
| Frontend node_modules | 500 MB      | Frontend/next-app/   |
| **Total**             | **~5.1 GB** |                      |

---

## âœ… Success Indicators

### **LLM Backend (Terminal 1):**

```
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
```

### **RAG Backend (Terminal 2):**

```
ğŸ‰ All critical models cached! Ready for offline operation.
INFO:     Uvicorn running on http://0.0.0.0:8001
```

### **Frontend (Terminal 3):**

```
âœ“ Ready in 2.3s
â—‹ Local:   http://localhost:3000
```

---

## ğŸ¬ Demo Workflow

1. **Start all services** (3 terminals)
2. **Open browser** â†’ http://localhost:3000
3. **Sign in:** admin@graphrag.com / admin123
4. **Upload documents** (PDF, DOCX, TXT)
5. **Chat with documents** (streaming responses)
6. **View knowledge graph** (Admin â†’ Graph Visualization)
7. **Export conversations** (JSON/Markdown)

---

## ğŸŒ Offline Mode

After first run, you can:

- âœ… Disconnect from internet
- âœ… Start all services
- âœ… Full RAG functionality works
- âœ… Upload, embed, retrieve, chat - all local!

**Perfect for:**

- Conference demos
- Client presentations
- Classroom teaching
- Air-gapped environments
- Travel/remote locations

---

## ğŸ¯ Port Summary

| Service                 | Port | URL                           |
| ----------------------- | ---- | ----------------------------- |
| Frontend                | 3000 | http://localhost:3000         |
| RAG Backend API         | 8001 | http://localhost:8001/v1/docs |
| LLM Backend             | 8080 | http://localhost:8080         |
| llama.cpp (if separate) | 8000 | http://localhost:8000         |

---

## ğŸ“š Documentation

- [AUTO_CACHE_SETUP.md](AUTO_CACHE_SETUP.md) - Auto-caching explanation
- [OFFLINE_SETUP.md](OFFLINE_SETUP.md) - Complete offline guide
- [TESSERACT_INSTALL.md](TESSERACT_INSTALL.md) - OCR setup (optional)
- [README.md](README.md) - Full project docs

---

## ğŸ†˜ Need Help?

1. **Check test results:** `cd RAG-Backend && test.bat`
2. **View logs:** Each terminal shows error messages
3. **Verify ports:** `netstat -ano | findstr "3000 8001 8080"`
4. **Re-cache models:** `cd RAG-Backend && cache.bat`

---

## ğŸ‰ You're Ready!

**System Status:**

- âœ… Auto-caching configured
- âœ… Offline mode enabled
- âœ… All models integrated
- âœ… Production-ready
- âœ… Demo-ready

**Just run the 3 terminals and you're live!** ğŸš€
