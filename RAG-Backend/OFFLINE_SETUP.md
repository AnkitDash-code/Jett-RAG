# RAG Backend - Offline Setup Guide

## ğŸš€ Quick Start (Automatic Model Caching)

### **First Time Setup (While Online)**

The RAG Backend will **automatically download and cache all models** when you run it for the first time:

```bash
cd RAG-Backend
myenv\Scripts\activate
python -m uvicorn app.main:app --host 0.0.0.0 --port 8001
```

On first startup, you'll see:

```
ğŸš€ RAG Backend - Model Caching (First Run Setup)
This will download ~175 MB of models for offline operation.
This only happens once. Please wait...

ğŸ“¥ Caching embedding model (all-MiniLM-L6-v2)...
âœ… Embedding model cached (dim: 384)
ğŸ“¥ Caching reranker model (ms-marco-MiniLM-L-6-v2)...
âœ… Reranker model cached
ğŸ“¥ Caching spaCy model (en_core_web_sm)...
âœ… spaCy model downloaded and cached
âœ… LLM model found: 4.37 GB
âš ï¸  Tesseract OCR not found (optional for OCR features)

ğŸ‰ All critical models cached! Ready for offline operation.
```

**That's it!** The models are now cached and you can run offline.

---

## ğŸ”§ Manual Model Caching (Optional)

If you want to pre-cache models before starting the server:

```bash
cd RAG-Backend
myenv\Scripts\activate
python cache_models.py
```

This explicitly downloads all models without starting the server.

---

## âœ… Verify Offline Readiness

Test that all models and services work:

```bash
cd RAG-Backend
myenv\Scripts\activate
python test_offline.py
```

**Expected Output:**

```
ğŸ§ª RAG Backend - Offline Readiness Test

ğŸ“¦ Testing Cached Models:
ğŸ§ª Testing Embedding Model...
   âœ… Embedding model working (dimension: 384)
ğŸ§ª Testing Reranker Model...
   âœ… Reranker model working (score: 0.1234)
ğŸ§ª Testing spaCy NER Model...
   âœ… spaCy model working (found 3 entities)
ğŸ§ª Testing Tesseract OCR...
   âš ï¸  Tesseract OCR not found (optional)

ğŸŒ Testing Services:
ğŸ§ª Testing LLM Backend connection...
   âœ… LLM Backend responding on port 8080
ğŸ§ª Testing RAG Backend connection...
   âœ… RAG Backend responding on port 8001
ğŸ§ª Testing Frontend connection...
   âœ… Frontend responding on port 3000

ğŸ“Š Test Summary
Models:
  âœ… Embedding Model
  âœ… Reranker Model
  âœ… spaCy Model
  âš ï¸  Tesseract OCR

Services:
  âœ… LLM Backend (8080)
  âœ… RAG Backend (8001)
  âœ… Frontend (3000)

ğŸ‰ All systems operational! Ready for offline demo.
```

---

## ğŸƒ Complete Offline Startup

### **1. Start LLM Backend** (Terminal 1)

```bash
cd LLM-Backend
myenv\Scripts\activate
python main.py
```

âœ… Should show: `Uvicorn running on http://0.0.0.0:8080`

### **2. Start RAG Backend** (Terminal 2)

```bash
cd RAG-Backend
myenv\Scripts\activate
python -m uvicorn app.main:app --host 0.0.0.0 --port 8001
```

âœ… First run will cache models, subsequent runs will skip (instant start)

### **3. Start Frontend** (Terminal 3)

```bash
cd Frontend/next-app
npm run dev
```

âœ… Should show: `Ready on http://localhost:3000`

---

## ğŸ“¦ What Gets Cached?

| Component                  | Size    | Purpose               | Required    |
| -------------------------- | ------- | --------------------- | ----------- |
| **all-MiniLM-L6-v2**       | ~80 MB  | Document embeddings   | âœ… Yes      |
| **ms-marco-MiniLM-L-6-v2** | ~80 MB  | Reranking results     | âœ… Yes      |
| **en_core_web_sm**         | ~15 MB  | Entity extraction     | âœ… Yes      |
| **mistral-7b-instruct**    | ~4.4 GB | LLM generation        | âœ… Yes      |
| **Tesseract OCR**          | ~50 MB  | Image text extraction | âš ï¸ Optional |

**Total Required:** ~4.6 GB  
**Cache Location:** `C:\Users\ASUS\.cache\huggingface\hub\`

---

## ğŸ” Troubleshooting

### **Models Don't Download**

```bash
# Manually cache all models
cd RAG-Backend
python cache_models.py
```

### **Check What's Missing**

```bash
python test_offline.py
```

### **Re-cache Models**

```bash
# Delete cache status file
del .model_cache_status

# Restart server (will re-download)
python -m uvicorn app.main:app --port 8001
```

### **Offline Mode Test**

1. Cache all models (while online)
2. Disconnect from internet
3. Run `test_offline.py`
4. Start all services
5. Should work completely offline âœ…

---

## ğŸ¯ Model Cache Status

The system tracks whether models are cached in:

```
RAG-Backend/.model_cache_status
```

- **Exists:** Models cached, skip download
- **Missing:** First run, download models

To force re-download, delete this file.

---

## ğŸ“ Architecture

```
Internet (First Run Only)
    â†“ (Downloads models)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Cache Dir    â”‚
â”‚  ~/.cache/...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (Loads from cache)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG Backend       â”‚â”€â”€â”€â”€â†’â”‚   LLM Backend   â”‚â”€â”€â”€â”€â†’â”‚  llama.cpp   â”‚
â”‚   (Port 8001)       â”‚     â”‚   (Port 8080)   â”‚     â”‚  (Port 8000) â”‚
â”‚                     â”‚     â”‚                 â”‚     â”‚              â”‚
â”‚ â€¢ Embeddings        â”‚     â”‚ â€¢ Mistral-7B    â”‚     â”‚ â€¢ GGUF Model â”‚
â”‚ â€¢ Reranker          â”‚     â”‚ â€¢ OpenAI API    â”‚     â”‚ â€¢ Local Gen  â”‚
â”‚ â€¢ spaCy NER         â”‚     â”‚                 â”‚     â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend          â”‚
â”‚   (Port 3000)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Offline Operation:**

- All models loaded from local cache
- No internet required after first setup
- Complete RAG pipeline runs locally

---

## âœ¨ Features

- âœ… **Automatic caching** on first run
- âœ… **Zero configuration** - just start the server
- âœ… **Offline-ready** after initial setup
- âœ… **Smart detection** - skips re-download
- âœ… **Comprehensive testing** - verify before demo
- âœ… **Clear logging** - see exactly what's happening

---

## ğŸ‰ Ready to Demo!

After first run:

1. âœ… All models cached (~4.6 GB)
2. âœ… No internet needed
3. âœ… Start services in any order
4. âœ… Full RAG pipeline works offline
5. âœ… Upload documents, chat, retrieve - all local!

**Demo anywhere - no WiFi needed!** ğŸš€
